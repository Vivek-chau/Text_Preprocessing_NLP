{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362d6079-f5a6-468c-8bf3-3024e41c622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91987\\anaconda3\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a28a1ab-5e9a-42ec-b02a-dcdc9b7660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus  = \"\"\"Hello Welcome, to Vivek Kumar's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b03247e-6a97-4491-8816-33e54dffa87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91987\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcc5fbc-8e85-456e-97b4-5f1e0d9edc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## paragraphs-->sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ee4de6-21f5-47dd-8d50-e98789b6a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Vivek Kumar's NLP Tutorials.\n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a36084d-5d1d-414a-b86e-6fbc4c37e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89f5485-f945-45fa-8750-a8c6af471fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a91bbaf-ff3a-4d13-9d63-96c5fa6600ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Vivek Kumar's NLP Tutorials.\n",
      "Please do watch the entire course!\n",
      "to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f82c4ee-ddc8-4ec6-9f45-5e5a83b65c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph -->words\n",
    "## sentence -->words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1917388-7f1d-48f3-92c3-f896a401c0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Vivek',\n",
       " 'Kumar',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus) # 's not considered as a individual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be3173d-067a-4966-84de-26d985091ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Vivek', 'Kumar', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "744db723-c620-4340-8150-cc3f7627b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1319f5-42a5-499d-ace2-b99f40b9a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Vivek',\n",
       " 'Kumar',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus) # 's is now splitted as a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58205ebf-3c09-468f-b08b-08d5487f5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aafbb41-02d6-4d0c-9e4b-c5a1472350c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8efd54-5982-4b10-b833-7585ceb610fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Vivek',\n",
       " 'Kumar',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus) # here . fullstop noe treated as a separate word but last fullstop of paragraph considered as a separate word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a431bc-12ae-4c06-a245-5115372d4da0",
   "metadata": {},
   "source": [
    "### Practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17711b08-2e6a-46bb-b777-c2181fd267b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1= \"\"\"The sun dipped below the horizon, casting a warm golden glow over the tranquil landscape. A gentle breeze rustled through the leaves, carrying with it the faint scent of blooming flowers. As twilight settled in, the sky transformed into a canvas of vibrant hues, painting a perfect end to the day.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e0cf1d1-d705-4310-8708-845f508af921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1c126c-3efd-4911-b98b-b7e4c52b1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here we are converting paragraph(corpus) into sentences(documents) \n",
    "\n",
    "docu = sent_tokenize(corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852601e4-d520-41a9-94ff-cb7b6fd43322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun dipped below the horizon, casting a warm golden glow over the tranquil landscape.\n",
      "A gentle breeze rustled through the leaves, carrying with it the faint scent of blooming flowers.\n",
      "As twilight settled in, the sky transformed into a canvas of vibrant hues, painting a perfect end to the day.\n"
     ]
    }
   ],
   "source": [
    "for sentence in docu:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe9928f2-fee7-4b9a-9a08-0a636f531ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now converting sentence into words\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ff7d10b-7ed7-42c3-b204-6b821a493e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'dipped', 'below', 'the', 'horizon', ',', 'casting', 'a', 'warm', 'golden', 'glow', 'over', 'the', 'tranquil', 'landscape', '.']\n",
      "['A', 'gentle', 'breeze', 'rustled', 'through', 'the', 'leaves', ',', 'carrying', 'with', 'it', 'the', 'faint', 'scent', 'of', 'blooming', 'flowers', '.']\n",
      "['As', 'twilight', 'settled', 'in', ',', 'the', 'sky', 'transformed', 'into', 'a', 'canvas', 'of', 'vibrant', 'hues', ',', 'painting', 'a', 'perfect', 'end', 'to', 'the', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in docu:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398842bc-6819-4bce-9791-b4e82d13f730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
